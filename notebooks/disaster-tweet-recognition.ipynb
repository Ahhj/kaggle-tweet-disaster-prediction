{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook proposes a model to predict binary labels for tweets to indicate whether the tweet refers to a disaster or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlp-getting-started/train.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n",
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "input_path = Path('/kaggle/input')\n",
    "for dirname, _, filenames in os.walk(input_path):\n",
    "    for filename in filenames:\n",
    "        print(Path(dirname) / filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "Read data from csvs provided into pandas.DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = input_path / 'nlp-getting-started'\n",
    "train_df = pd.read_csv(data_path / 'train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(data_path / 'test.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a quick look at missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows missing keyword: 0.80%\n",
      "Rows missing keyword with target = 1: 1.28%\n",
      "Rows missing keyword with target = 0: 0.44%\n",
      "\n",
      "Rows missing location: 33.27%\n",
      "Rows missing location with target = 1: 32.86%\n",
      "Rows missing location with target = 0: 33.58%\n"
     ]
    }
   ],
   "source": [
    "print(f'Rows missing keyword: {train_df.keyword.isnull().mean() * 100:.2f}%')\n",
    "print(f'Rows missing keyword with target = 1: {train_df.loc[train_df.target==1].keyword.isnull().mean() * 100:.2f}%')\n",
    "print(f'Rows missing keyword with target = 0: {train_df.loc[train_df.target==0].keyword.isnull().mean() * 100:.2f}%')\n",
    "\n",
    "print('')\n",
    "print(f'Rows missing location: {train_df.location.isnull().mean() * 100:.2f}%')\n",
    "print(f'Rows missing location with target = 1: {train_df.loc[train_df.target==1].location.isnull().mean() * 100:.2f}%')\n",
    "print(f'Rows missing location with target = 0: {train_df.loc[train_df.target==0].location.isnull().mean() * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notes__:\n",
    "* A small fraction of rows are missing a keyword\n",
    "* A third of rows are missing a location. Also from above location data is quite dirty.\n",
    "* There aren't particularly noticeable differences in either case between target = 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Clean tweets and transform to sequences of integers for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>words</th>\n",
       "      <th>stop_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason thi earthquak may allah forgiv us</td>\n",
       "      <td>[deed, reason, thi, earthquak, may, allah, for...</td>\n",
       "      <td>[our, are, the, of, all]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n",
       "      <td>[all, to, in, are, be, by, no, other, or, in, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>peopl receiv wildfir evacu order california</td>\n",
       "      <td>[peopl, receiv, wildfir, evacu, order, califor...</td>\n",
       "      <td>[in]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent thi photo rubi alaska smoke wildfir p...</td>\n",
       "      <td>[got, sent, thi, photo, rubi, alaska, smoke, w...</td>\n",
       "      <td>[just, from, as, from, into, a]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                       cleaned_text  \\\n",
       "0       1      deed reason thi earthquak may allah forgiv us   \n",
       "1       1               forest fire near la rong sask canada   \n",
       "2       1  resid ask shelter place notifi offic evacu she...   \n",
       "3       1        peopl receiv wildfir evacu order california   \n",
       "4       1  got sent thi photo rubi alaska smoke wildfir p...   \n",
       "\n",
       "                                               words  \\\n",
       "0  [deed, reason, thi, earthquak, may, allah, for...   \n",
       "1       [forest, fire, near, la, rong, sask, canada]   \n",
       "2  [resid, ask, shelter, place, notifi, offic, ev...   \n",
       "3  [peopl, receiv, wildfir, evacu, order, califor...   \n",
       "4  [got, sent, thi, photo, rubi, alaska, smoke, w...   \n",
       "\n",
       "                                          stop_words  \n",
       "0                           [our, are, the, of, all]  \n",
       "1                                                 []  \n",
       "2  [all, to, in, are, be, by, no, other, or, in, ...  \n",
       "3                                               [in]  \n",
       "4                    [just, from, as, from, into, a]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_number(word): \n",
    "        try:\n",
    "            float(word.replace(',', ''))\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "def preprocess_text(raw_df):\n",
    "    df = raw_df.copy()\n",
    "    \n",
    "    # Replace mentions & links\n",
    "    df['cleaned_text'] = df.text.str.replace('@\\S+', 'mention')\n",
    "    df.cleaned_text = df.cleaned_text.str.replace('http\\S+', 'http')\n",
    "    \n",
    "    # Remove hash from hashtag\n",
    "    df.cleaned_text = df.cleaned_text.str.replace('#', '')\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True)\n",
    "    df['words'] = df.cleaned_text.apply(tokenizer.tokenize)\n",
    "\n",
    "    # Remove punctuation\n",
    "    df.words = df.words.apply(lambda word_list: [w for w in word_list if w not in string.punctuation])\n",
    "\n",
    "    # Stemming\n",
    "    ps = PorterStemmer()\n",
    "    df.words = df.words.apply(lambda word_list: [ps.stem(w) for w in word_list])\n",
    "\n",
    "    # Split stop words and rest\n",
    "    df['stop_words'] = df.words.apply(lambda word_list: [w for w in word_list if w in set(stopwords.words('english'))])\n",
    "    df.words = df.words.apply(lambda word_list: [w for w in word_list if w not in set(stopwords.words('english'))])\n",
    "\n",
    "    # Remove numbers\n",
    "    df.words = df.words.apply(lambda word_list: [w for w in word_list if not is_number(w)])\n",
    "    \n",
    "    # Get cleaned text\n",
    "    df.cleaned_text = df.words.apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = preprocess_text(train_df)\n",
    "test_df = preprocess_text(test_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>words</th>\n",
       "      <th>stop_words</th>\n",
       "      <th>cleaned_keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason thi earthquak may allah forgiv us</td>\n",
       "      <td>[deed, reason, thi, earthquak, may, allah, for...</td>\n",
       "      <td>[our, are, the, of, all]</td>\n",
       "      <td>no%20keyword</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "      <td>[]</td>\n",
       "      <td>no%20keyword</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n",
       "      <td>[all, to, in, are, be, by, no, other, or, in, ...</td>\n",
       "      <td>no%20keyword</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>peopl receiv wildfir evacu order california</td>\n",
       "      <td>[peopl, receiv, wildfir, evacu, order, califor...</td>\n",
       "      <td>[in]</td>\n",
       "      <td>no%20keyword</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent thi photo rubi alaska smoke wildfir p...</td>\n",
       "      <td>[got, sent, thi, photo, rubi, alaska, smoke, w...</td>\n",
       "      <td>[just, from, as, from, into, a]</td>\n",
       "      <td>no%20keyword</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                       cleaned_text  \\\n",
       "0       1      deed reason thi earthquak may allah forgiv us   \n",
       "1       1               forest fire near la rong sask canada   \n",
       "2       1  resid ask shelter place notifi offic evacu she...   \n",
       "3       1        peopl receiv wildfir evacu order california   \n",
       "4       1  got sent thi photo rubi alaska smoke wildfir p...   \n",
       "\n",
       "                                               words  \\\n",
       "0  [deed, reason, thi, earthquak, may, allah, for...   \n",
       "1       [forest, fire, near, la, rong, sask, canada]   \n",
       "2  [resid, ask, shelter, place, notifi, offic, ev...   \n",
       "3  [peopl, receiv, wildfir, evacu, order, califor...   \n",
       "4  [got, sent, thi, photo, rubi, alaska, smoke, w...   \n",
       "\n",
       "                                          stop_words cleaned_keyword  \n",
       "0                           [our, are, the, of, all]    no%20keyword  \n",
       "1                                                 []    no%20keyword  \n",
       "2  [all, to, in, are, be, by, no, other, or, in, ...    no%20keyword  \n",
       "3                                               [in]    no%20keyword  \n",
       "4                    [just, from, as, from, into, a]    no%20keyword  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem_keyword(keyword):\n",
    "    ps = PorterStemmer()\n",
    "    # Handle split keywords\n",
    "    words = keyword.split('%20')\n",
    "    return '%20'.join([ps.stem(w) for w in words])\n",
    "\n",
    "def preprocess_keywords(raw_df):\n",
    "    df = raw_df.copy()\n",
    "    df['cleaned_keyword'] = df.keyword.fillna('no%20keyword')\n",
    "    df.cleaned_keyword = df.cleaned_keyword.apply(stem_keyword)\n",
    "    return df\n",
    "\n",
    "train_df = preprocess_keywords(train_df)\n",
    "test_df = preprocess_keywords(test_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use prepocessed text and keywords to get inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get columns of training set to form inputs and outputs.\n",
    "train_text = train_df.cleaned_text.values\n",
    "train_keywords = train_df.cleaned_keyword.values\n",
    "train_targets = train_df.target.values\n",
    "\n",
    "# Get columns of test set to form inputs and outputs.\n",
    "test_text = test_df.cleaned_text.values\n",
    "test_keywords = test_df.cleaned_keyword.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & validation\n",
    "Train and validate a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorise text\n",
    "word_vec = TfidfVectorizer(\n",
    "    ngram_range=(1, 1),\n",
    "    max_df=0.99,\n",
    "    min_df=2,\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=False,\n",
    "    norm='l2'\n",
    ")\n",
    "word_vec.fit(np.hstack([train_text, test_text]))\n",
    "train_text_transf = word_vec.transform(train_text).toarray()\n",
    "test_text_transf = word_vec.transform(test_text).toarray()\n",
    "\n",
    "# Vectorise keywords\n",
    "keyword_vec = TfidfVectorizer(\n",
    "    ngram_range=(1, 1),\n",
    "    max_df=0.99,\n",
    "    min_df=2,\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=False,\n",
    "    norm='l1'\n",
    ")\n",
    "keyword_vec.fit(np.hstack([train_keywords, test_keywords]))\n",
    "train_keywords_transf = word_vec.transform(train_keywords).toarray()\n",
    "test_keywords_transf = word_vec.transform(test_keywords).toarray()\n",
    "\n",
    "# Model inputs\n",
    "train_inputs = np.hstack([train_keywords_transf, train_text_transf])\n",
    "test_inputs = np.hstack([test_keywords_transf, test_text_transf])\n",
    "\n",
    "# Split training and validataion sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_inputs, train_targets, test_size=0.2, stratify=train_targets)\n",
    "X_test = test_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/100\n",
      "6090/6090 [==============================] - 4s 603us/step - loss: 0.6782 - accuracy: 0.5729 - val_loss: 0.6588 - val_accuracy: 0.5936\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65881, saving model to 20200127215220_best_model.hdf5\n",
      "Epoch 2/100\n",
      "6090/6090 [==============================] - 3s 529us/step - loss: 0.6331 - accuracy: 0.6548 - val_loss: 0.6072 - val_accuracy: 0.6907\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.65881 to 0.60720, saving model to 20200127215220_best_model.hdf5\n",
      "Epoch 3/100\n",
      "6090/6090 [==============================] - 3s 527us/step - loss: 0.5790 - accuracy: 0.7243 - val_loss: 0.5473 - val_accuracy: 0.7551\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.60720 to 0.54731, saving model to 20200127215220_best_model.hdf5\n",
      "Epoch 4/100\n",
      "6090/6090 [==============================] - 3s 515us/step - loss: 0.5234 - accuracy: 0.7611 - val_loss: 0.5082 - val_accuracy: 0.7649\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.54731 to 0.50823, saving model to 20200127215220_best_model.hdf5\n",
      "Epoch 5/100\n",
      "6090/6090 [==============================] - 3s 540us/step - loss: 0.4925 - accuracy: 0.7741 - val_loss: 0.4932 - val_accuracy: 0.7636\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.50823 to 0.49323, saving model to 20200127215220_best_model.hdf5\n",
      "Epoch 6/100\n",
      "6090/6090 [==============================] - 3s 542us/step - loss: 0.4789 - accuracy: 0.7846 - val_loss: 0.4863 - val_accuracy: 0.7735\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.49323 to 0.48634, saving model to 20200127215220_best_model.hdf5\n",
      "Epoch 7/100\n",
      "6090/6090 [==============================] - 3s 545us/step - loss: 0.4602 - accuracy: 0.7962 - val_loss: 0.4786 - val_accuracy: 0.7807\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.48634 to 0.47860, saving model to 20200127215220_best_model.hdf5\n",
      "Epoch 8/100\n",
      "6090/6090 [==============================] - 3s 545us/step - loss: 0.4469 - accuracy: 0.8007 - val_loss: 0.4746 - val_accuracy: 0.7774\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.47860 to 0.47461, saving model to 20200127215220_best_model.hdf5\n",
      "Epoch 9/100\n",
      "6090/6090 [==============================] - 3s 572us/step - loss: 0.4440 - accuracy: 0.8008 - val_loss: 0.4709 - val_accuracy: 0.7833\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.47461 to 0.47090, saving model to 20200127215220_best_model.hdf5\n",
      "Epoch 10/100\n",
      "6090/6090 [==============================] - 3s 521us/step - loss: 0.4347 - accuracy: 0.8082 - val_loss: 0.4691 - val_accuracy: 0.7840\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.47090 to 0.46909, saving model to 20200127215220_best_model.hdf5\n",
      "Epoch 11/100\n",
      "6090/6090 [==============================] - 3s 556us/step - loss: 0.4161 - accuracy: 0.8189 - val_loss: 0.4710 - val_accuracy: 0.7833\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.46909\n",
      "Epoch 12/100\n",
      "6090/6090 [==============================] - 3s 555us/step - loss: 0.4166 - accuracy: 0.8177 - val_loss: 0.4687 - val_accuracy: 0.7833\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.46909 to 0.46871, saving model to 20200127215220_best_model.hdf5\n",
      "Epoch 13/100\n",
      "6090/6090 [==============================] - 3s 564us/step - loss: 0.4081 - accuracy: 0.8250 - val_loss: 0.4678 - val_accuracy: 0.7873\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.46871 to 0.46776, saving model to 20200127215220_best_model.hdf5\n",
      "Epoch 14/100\n",
      "6090/6090 [==============================] - 3s 555us/step - loss: 0.3986 - accuracy: 0.8274 - val_loss: 0.4688 - val_accuracy: 0.7886\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.46776\n",
      "Epoch 15/100\n",
      "6090/6090 [==============================] - 3s 515us/step - loss: 0.3964 - accuracy: 0.8256 - val_loss: 0.4672 - val_accuracy: 0.7938\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.46776 to 0.46716, saving model to 20200127215220_best_model.hdf5\n",
      "Epoch 16/100\n",
      "6090/6090 [==============================] - 3s 520us/step - loss: 0.3998 - accuracy: 0.8245 - val_loss: 0.4683 - val_accuracy: 0.7886\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.46716\n",
      "Epoch 17/100\n",
      "6090/6090 [==============================] - 3s 514us/step - loss: 0.3870 - accuracy: 0.8343 - val_loss: 0.4662 - val_accuracy: 0.7905\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.46716 to 0.46620, saving model to 20200127215220_best_model.hdf5\n",
      "Epoch 18/100\n",
      "6090/6090 [==============================] - 3s 525us/step - loss: 0.3776 - accuracy: 0.8333 - val_loss: 0.4691 - val_accuracy: 0.7919\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.46620\n",
      "Epoch 19/100\n",
      "6090/6090 [==============================] - 3s 518us/step - loss: 0.3776 - accuracy: 0.8374 - val_loss: 0.4676 - val_accuracy: 0.7938\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.46620\n",
      "Epoch 20/100\n",
      "6090/6090 [==============================] - 3s 512us/step - loss: 0.3671 - accuracy: 0.8399 - val_loss: 0.4672 - val_accuracy: 0.7925\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.46620\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f548a5e1c88>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop early and save the best model\n",
    "model_path = f'{datetime.utcnow():%Y%m%d%H%M%S}_best_model.hdf5'\n",
    "check_point = ModelCheckpoint(model_path, monitor = \"val_loss\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3)\n",
    "\n",
    "# Build sequential model\n",
    "model = Sequential()\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.9))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fitting & validation on training set.\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_val, y_val),\n",
    "          batch_size=100,\n",
    "          epochs=100,\n",
    "          callbacks=[early_stop, check_point])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions\n",
    "Retrain model, generate predictions and submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, model):\n",
    "    return (model.predict(X) > 0.5).ravel().astype(int)\n",
    "\n",
    "y_pred = predict(X_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       0\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub_df = pd.read_csv('../input/nlp-getting-started/sample_submission.csv')\n",
    "\n",
    "sub_df = pd.DataFrame({\n",
    "    'id': sample_sub_df['id'].values.tolist(),\n",
    "    'target': y_pred.ravel()\n",
    "})\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
